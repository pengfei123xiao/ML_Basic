{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/pengfei123xiao/ML_Basic/blob/master/models/Ch05-DecisionTree/RF_GBDT_XGBoost_LightGBM/XGBoost_Key_Points.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xkKLpnv8R1xI"
   },
   "source": [
    "## 0.CART树\n",
    "\n",
    "0.1 CART是决策树算法之一，它可用于分类问题也可用于回归问题。它与ID3、C4.5等算法不同的是，在节点分支的时候，它每次仅分成左、右两个节点，即它假设决策树是二叉树。另外，它在选择最优分割变量与最优分割点的准则也有所不同。\n",
    "\n",
    "0.2 CART树的生成就是递归地构建二叉决策树的过程。对**回归树用平方误差最小化准则**，对**分类树用基尼指数最小化准则**，进行特征选择，生成二叉树。\n",
    "\n",
    "![cart](https://raw.githubusercontent.com/xmj-datawhale/adv-algorithm/master/img/xgboost-cart.jpg)\n",
    "\n",
    "在这里，平方误差就是叶子节点的输出值与样本实际值偏差的平方和，概念比较清晰不过多赘述。至于基尼指数，通俗来讲就是，在一个分支里，随机抽取两个样本，这两个样本所属类别不一样的概率，如果基尼指数越小，那么这个分支的纯度就越高。具体看看基尼指数怎么定义的：       分类问题中，假设有K个类，样本点属于k类的概率为  ，则概率分布的基尼指数定义为：\n",
    "\n",
    "$Gini(p)=\\sum_{k=1}^{K}{p_{k}(1-p_{k})=1-\\sum_{k=1}^{K}{p_{k}^{2}}}$\n",
    "\n",
    "对于给定的样本集合D，其基尼指数为：\n",
    "\n",
    "$Gini(D)=1-\\sum_{k=1}^{K}{(\\frac{ |c_{k}|}{|D|})^{2}}$\n",
    "\n",
    "这里， $C_k$ 是D中属于第k类的样本子集，K是类的个数。\n",
    "\n",
    "如果样本集合D根据特征A是否取某一可能值a被分割成 $D_1$ 和 $D_2$ 两部分，即\n",
    "\n",
    "$D_1=\\{(x,y)\\in D|A(x)=a\\},D_{2}=D-D_{1}$\n",
    "\n",
    "则在特征A条件下，集合D的基尼指数定义为：\n",
    "\n",
    "$Gain(D,A)=\\frac{|D_{1}|}{|D|}Gini(D_{1})+\\frac{|D_{2}|}{|D|}Gini(D_{2})$\n",
    "\n",
    "基尼指数Gini(D)表示集合的不确定性，基尼指数Gini(D,A)表示经A=a分割后集合D的不确定性。基尼指数值越大，样本集合的不确定性也就越大。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bGf11Vbl0N3B"
   },
   "source": [
    "## 1.算法原理\n",
    "1.1.算法思想\n",
    "\n",
    "- XGBoost算法的基本思想与GBDT类似，不断地进行特征分裂来生长一棵树，每一轮学习一棵树，其实就是去**拟合上一轮模型的预测值与实际值之间的残差**。当我们训练完成得到k棵树时，我们要预测一个样本的分数，其实就是根据这个样本的特征，**在每棵树中会找到对应的一个叶子节点**，每个叶子节点就对应一个分数，最后只需将每棵树对应的分数加起来就是该样本的预测值。\n",
    "\n",
    "1.2.模型的函数形式\n",
    "\n",
    "给定数据集D={ $x_i,y_i$}，XGBoost进行additive training，学习K棵树，采用以下函数对样本进行预测：\n",
    "\n",
    "$\\tilde{y}=\\phi(x_{i})=\\sum_{k=1}^{K}{f_{k}(x_{i})}，f_{k}\\in\\Gamma$\n",
    "\n",
    "这里 $\\Gamma$ 是假设空间，f(x)是回归树：\n",
    "$\\Gamma=\\{ f(x)=w_{q(x)}\\}(q:R^m \\rightarrow T,w \\in R^T)$\n",
    "\n",
    "q(x)表示样本x分到了某个叶子节点上，w是叶子节点的分数，所以$w_{q(x)}$ 表示回归树对样本的预测值。\n",
    "\n",
    "例子：预测一个人是否喜欢玩泥沙:\n",
    "\n",
    "![cart](https://raw.githubusercontent.com/xmj-datawhale/adv-algorithm/master/img/gbdt_tree.jpg)\n",
    "\n",
    "上面训练出了2棵树，小男孩的预测分数就是两棵树中小男孩所落到的节点的分数相加，老头子的分数同理。\n",
    "\n",
    "回归树预测输出的是实数分数，可用于回归、分类、排序等任务。对于回归问题，预测分数可以直接作为目标值，**对于分类问题，需要映射成概率，比如采用逻辑函数** $\\sigma(z)=-\\frac{1}{1+e^{-z}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "L0e9Ojlf0T4U"
   },
   "source": [
    "## 2.损失函数\n",
    "\n",
    "2.1 每一轮训练一棵树，都是为了使得损失函数能够极小化，也就是每一棵树训练要达到的目标。XGBoost的损失函数（目标函数）与GBDT不同，它不**仅仅是衡量了模型的拟合误差，还增加了正则化项**，即对每棵树树复杂度的惩罚项，来限制树的复杂度，防止过拟合。\n",
    "\n",
    "**参数空间中的目标函数：**\n",
    "\n",
    "![cart](https://raw.githubusercontent.com/xmj-datawhale/adv-algorithm/master/img/xgboost_target_func.jpg)\n",
    "\n",
    "在函数空间中目标函数形式为： $L(\\phi)=\\displaystyle \\sum_i l(\\tilde y_i,y_i)+ \\displaystyle \\sum_k \\Omega(f_k)$\n",
    "\n",
    "其中，正则化项 $\\Omega(f)=\\gamma T+\\frac{1}{2}\\lambda||w||^2$ ，T为叶子节点个数，w为叶子节点分数。\n",
    "\n",
    "误差函数的二阶泰勒展开：\n",
    "\n",
    "- 第t次迭代后，模型的预测等于前t-1次模型预测加第t棵树的预测：$\\hat y_i^{(t)}=\\hat y^{(t-1)}+f_t(x_i)$\n",
    "\n",
    "- 此时目标函数可写作：$L^{(t)}=\\displaystyle \\sum^n_{i=1} l(y_i,\\hat y_i^{t-1}+f_t(x_i))+\\Omega(f_t)$\n",
    "   \n",
    "公式中， $y_i,\\tilde y_i^{t-1}$ 都已知，模型要学习的只有第t棵树 f_t\n",
    "\n",
    "将误差函数在 $y_i^{t-1}$ 处进行二阶泰勒展开：$L^{(t)} \\approx \\displaystyle \\sum_{i=1}^n [l(y_i,\\hat y^{(t-1)})+g_i f_t(x_i)+\\frac{1}{2}h_i f_t^2(x_i)]+\\Omega(f_t)$\n",
    "- 公式中\n",
    "\n",
    "$g_i=\\partial_{\\hat y^{(t-1)}}l(y_i,\\hat y^{(t-1)}) $ $h_i-\\partial^2_{\\hat y_{(t-1)}}l(y_i,\\hat y^{(t-1)})$\n",
    "- 把公式中的常数项去掉，得到：$\\tilde L^{(t)}=\\displaystyle \\sum_{i=1}^n[g_if_t(x_i)_\\frac{1}{2}h_if_t^2(x_i)]+\\Omega(f_t)$\n",
    "- 把 $f_t,\\Omega(f_t)$ 写成树结构的形式，即把下式带入目标函数中$f(x)=w_{q(x)} \\quad \\Omega(f)=\\gamma T+\\frac{1}{2}\\lambda||w||^2$\n",
    "- 得到：$\\tilde L^{(t)}=\\displaystyle \\sum_{i=1}^n[g_if_t(x_i)_\\frac{1}{2}h_if_t^2(x_i)]+\\Omega(f_t)$\n",
    "\n",
    "=$\\displaystyle\\sum_{i=1}^n[g_iw_{q(x_i)}+\\frac{1}{2}h_iw^2_q(x_i)]+\\gamma T +\\lambda\\frac{1}{2}\\displaystyle\\sum_{j=1}^Tw_j^2$\n",
    "![formula](https://raw.githubusercontent.com/xmj-datawhale/adv-algorithm/master/img/xgboost_formula.jpg)\n",
    "- 怎么统一起来呢？\n",
    "定义每个叶节点j上的样本集合为 $I_j=\\{i|q(x_i)=j\\}$ ，则目标函数可以写成按叶节点累加的形式：$\\tilde L^{(t)}=\\displaystyle\\sum_{j=1}^T[(\\displaystyle\\sum_{i\\in I_j}g_i)w_j+\\frac{1}{2}(\\displaystyle\\sum_{i\\in I_j}h_i+\\lambda)w_j^2]+\\gamma T\\\\=\\displaystyle\\sum_{j=1}^T[G_jw_j+\\frac{1}{2}(H_j+\\lambda)w_j^2]+\\gamma T$\n",
    "- 如果确定了树的结构（即q(x)确定），为了使目标函数最小，可以令其导数为0，解得每个叶节点的最有预测分数为\n",
    "$w_j^*=-\\frac{G_j}{H_j+\\lambda}$\n",
    "- 代入目标函数，得到最小损失为：$\\tilde L^*=-\\frac{1}{2}\\displaystyle\\sum_{j=1}^T\\frac{G_j^2}{H_j+\\lambda}+\\gamma T$\n",
    "\n",
    "----------------\n",
    "squre loss: $L(y_i, \\hat{y_i})=(y_i- \\hat{y_i})^2$\n",
    "Logistic loss: $L(y_i, \\hat{y_i})=y_iIn(1+e^{-\\hat{y_i}})+(1-y_i)In(1+e^{-\\hat{y_i}})$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "y_98-18y0Y5T"
   },
   "source": [
    "## 3.分裂结点算法\n",
    "3.1 当回归树的结构确定时，我们已经推导出其最优的节点分数及对应的最小损失值，问题是怎么确定树的结构呢？\n",
    "- XGBoost的打分函数：\n",
    "![formula](https://raw.githubusercontent.com/xmj-datawhale/adv-algorithm/master/img/xgboost_loss_func.jpg)\n",
    "标红部分衡量了每个叶子节点对总体损失的贡献，我们希望损失函数越小越好，则标红部分的值越大越好。\n",
    "\n",
    "- 因此，对一个叶子节点进行分裂，分裂前后的增益定义为： $Gain=\\frac{G_{L}^{2}}{H_{L}+\\lambda}+ \\frac{G_{R}^{2}}{H_{R}+\\lambda}-\\frac{(G_{L}+G_{R})^{2}}{H_{L}+H_{R}+\\lambda}-\\gamma$\n",
    "![formula](https://raw.githubusercontent.com/xmj-datawhale/adv-algorithm/master/img/xgboost-formula-explain.png)\n",
    "\n",
    "\n",
    "\n",
    "Gain的值越大，分裂后L减少越多。所当对一个叶节点分割时，计算所有候选特征对应的Gain选取Gain最大的进行分割。\n",
    "- 1.精确算法：遍历所有特征的所有可能的分割点，计算gain值，选取值最大的特征去分割：\n",
    "![formula](https://raw.githubusercontent.com/xmj-datawhale/adv-algorithm/master/img/xgboost_algorithm_1.jpg)\n",
    "- 2.近似算法，对每个特征，只考察分位点，减少计算复杂度\n",
    "![formula](https://raw.githubusercontent.com/xmj-datawhale/adv-algorithm/master/img/xgboost_algorithm_2.jpg)\n",
    "Global：学习每棵树前，提出候选切分点\n",
    "\n",
    "Local：每次分列前，重新提出候选切分点\n",
    "- 实际上，XGBoost不是简单地按照样本个数进行分位，而是以二阶导数值作为权重。\n",
    "\n",
    "--------\n",
    "1. 暴力枚举：遍历所有特征的所有可能的分割点，计算Gain值，选取最大(Feature, label)去分裂\n",
    "2. 近似方法：对于每个特征，只考察分位点，减少计算复杂度"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MPNHNhe_0WQ0"
   },
   "source": [
    "## 4.正则化\n",
    "XGBoost有几种防止过拟合的正则化方法。\n",
    "\n",
    "4.1.在目标函数中，不仅包含了模型的拟合误差函数，还增加了关于每棵树复杂度的惩罚项，即叶子节点的个数以及叶子节点分数的平方项，限制了树的复杂度。\n",
    "\n",
    "4.2.像GBDT那样，可以对每个模型乘上一个步长a，a∈(0,1]，用来降低每个模型对预测的贡献。\n",
    "\n",
    "4.3.可以行采样与列采样，与随机森林类似。\n",
    "\n",
    "---------------\n",
    "在XGBoost中，是将树的复杂度作为正则项加入，优化器在工作时会尽量不让这个树更复杂。\n",
    "\n",
    "在给出n个实例，m维特征的情况下，\n",
    "$\\hat{y_i}=\\phi(x_i)=\\sum^K_{k=1}f_k(x_i), f_k \\in F$\n",
    "\n",
    "$q(x)$代表一颗树，$W$代表叶子的分数，$f(x)$为样本实例的预测值，所以和CART的区别在于每个叶子节点有相应的权重$W_i$。为了学到模型需要的函数，需要定义正则化目标函数\n",
    "$L(\\phi)=\\sum_il(\\hat{y_i}, y_i)+\\sum_K\\Omega(f_k), where \\Omega(f)=\\gamma T+\\frac{1}{2}\\lambda ||w||^2$\n",
    "一种标准的正则化目标项= differentiable convex loss function + regularization，即损失函数+正则项。\n",
    "\n",
    "$L$衡量预测值与真实值的差异，$Ω$作为模型复杂度的惩罚项，对于树的叶子节点个数和叶子节点权重的正则，防止过拟合，(simple is perfect)，正则化项比RGF模型更加简单。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7Z45QHei0ewv"
   },
   "source": [
    "### 5.对缺失值处理\n",
    "\n",
    "XGBoost在选择最优分裂点的时候，不考虑该特征的缺失样本,通过这个技巧来**减少了为稀疏离散特征寻找split point的时间开销**。但在后面对样本划分中，会分别将该特征的缺失样本放到左、右节点中，分别计算Gain值，**哪边大就把该样本划分到哪一边**。如果在训练集中，该特征没有出现缺失样本，但在**预测的时候出现缺失样本了，则默认将该样本划分到右节点中**。具体方法：\n",
    "\n",
    "![xgboost_fill_loss_feature](https://raw.githubusercontent.com/xmj-datawhale/adv-algorithm/master/img/xgboost_fill_loss_feature.jpg)\n",
    "\n",
    "------------------\n",
    "在XGBoost论文中关于缺失值的处理将其看作与稀疏矩阵的处理一样。在寻找split point的时候，不会对该特征为missing的样本进行遍历统计，只对该列特征值为non-missing的样本上对应的特征值进行遍历，通过这个技巧来减少了为稀疏离散特征寻找split point的时间开销。在逻辑实现上，为了保证完备性，会分别处理将missing该特征值的样本分配到左叶子结点和右叶子结点的两种情形，计算增益后**选择增益大的方向进行分裂即可。** 可以为缺失值或者指定的值指定分支的默认方向，这能大大提升算法的效率。如果在训练中没有缺失值而在预测中出现缺失，那么会自动将缺失值的划分方向放到右子树。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XPmbBnY70rdQ"
   },
   "source": [
    "## 6.优缺点\n",
    "1. 优点（较GBDT）\n",
    "- 将树模型的复杂度加入到正则项中，来避免过拟合，因此泛化性能会优于GBDT。\n",
    "- 损失函数用泰勒展开式展开，同时用到了一阶和二阶导数，可以加快优化速度。\n",
    "- GBDT只支持CART作为基学习器，XGBoost还支持线性分类器作为基学习器。\n",
    "- 引进了特征子采样，像随机森林那样，既能避免过拟合，又能减少计算。\n",
    "- 在寻找最优分割点时，考虑到传统的贪心算法效率较低，实现了一种近似贪心算法，用来加速和减少内存消耗，除此之外，还考虑了稀疏数据集合缺失值的处理。\n",
    "- XGBoost支持并行处理。XGBoost的并行不是模型生成的并行，而是在**特征上的并行**，将特征排序后以block的形式存储在内存中，在后面迭代重复使用这个结构。这个block也使得并行化成为了可能，其次在节点分裂时，计算每个特征的增益，最终选择增益最大的那个特征去做分割，那么**各个特征的增益计算就可以开多线程进行**。\n",
    "\n",
    "2. 缺点（较LightGBM）\n",
    "- XGBoosting采用预排序，在迭代之前，对结点的特征做预排序，遍历选择最优分割点，数据量大时，耗时且展空间。LightGBM方法采用histogram算法，占用的内存低，数据分割的复杂度更低。\n",
    "- XGBoosting采用level-wise生成决策树，**同时分裂同一层的叶子**，从而进行多线程优化，但很多叶子节点的分裂增益较低，没必要进行跟进一步的分裂，这就**带来了不必要的开销**，但是**不容易过拟合**；LightGBM采用深度优化，leaf-wise生长策略，每次从**当前叶子中选择增益最大的结点进行分裂**，循环迭代，但会生长出**更深的决策树**，产生过拟合。因此引入了一个**阈值**进行限制，防止过拟合。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uZ1TiKL70cZW"
   },
   "source": [
    "## 7.应用场景\n",
    "XGboost能够在一系列的问题上取得良好的效果，这些问题包括存销预测、物理事件分类、网页文本分类、顾客行为预测、点击率预测、动机探测、产品分类。多领域依赖数据分析和特征工程在这些结果中扮演重要的角色。XGBoost在所有场景中提供可扩展的功能，XGBoost可扩展性保证了相比其他系统更快速，XGBoost算法优势具体体现在：处理稀疏数据的新颖的树的学习算法、近似学习的分布式加权直方图。XGBoost能够基于外存的计算，保障了大数据的计算，使用少量的节点资源可处理大量的数据。XGBoost的主要贡献：\n",
    "\n",
    "- 构建了高可扩展的端到端的boosting系统。\n",
    "- 提出了具有合理理论支撑的分布分位调整框架。\n",
    "- 介绍了一个新颖的并行适应稀疏处理树学习算法。\n",
    "- 提出了基于缓存块的结构（cache-aware block structure）便于外存树（out-of-core tree）的学习。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uJl5VXy60pgT"
   },
   "source": [
    "## 8.Sklearn参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "V7V-9hXK3Pa5"
   },
   "outputs": [],
   "source": [
    "class xgboost.DMatrix(data, label=None, missing=None, weight=None, silent=False, \n",
    "                      feature_names=None, feature_types=None, nthread=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YZKK9iMt3Tp6"
   },
   "source": [
    "`data`: DMatrix的数据源。当数据是字符串类型时，它表示路径libsvm格式txt文件，或者能xgboost读取的二进制文件\n",
    "\n",
    "`label`：训练数据的标签\n",
    "\n",
    "`missing`：需要以缺失值的形式表示的数据中的值\n",
    "\n",
    "`weight`：每个实例的权重\n",
    "\n",
    "`silent`：是否在构建期间打印信息\n",
    "\n",
    "`feature_names`：为特性设置名称\n",
    "\n",
    "`feature_types`：为特性设置类别\n",
    "\n",
    "`nthread`：从numpy从加载数据的线程数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "v8zqgBVr0lhz"
   },
   "source": [
    "## 参考\n",
    "西瓜书\n",
    "\n",
    "cs229吴恩达机器学习课程\n",
    "\n",
    "李航统计学习\n",
    "\n",
    "https://www.cnblogs.com/wj-1314/p/9402324.html\n",
    "\n",
    "https://blog.csdn.net/a819825294/article/details/51206410\n",
    "\n",
    "https://zhuanlan.zhihu.com/p/38297689\n",
    "\n",
    "https://www.jiqizhixin.com/graph/technologies/ea0eb940-c873-42bc-a752-0a07f15a52c0\n",
    "\n",
    "公式推导参考：http://t.cn/EJ4F9Q0\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "XGBoost_Key_Points.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
