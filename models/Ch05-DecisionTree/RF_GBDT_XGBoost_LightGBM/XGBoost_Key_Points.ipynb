{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "XGBoost_Key_Points.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pengfei123xiao/ML_Basic/blob/master/models/Ch05-DecisionTree/RF_GBDT_XGBoost_LightGBM/XGBoost_Key_Points.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bGf11Vbl0N3B",
        "colab_type": "text"
      },
      "source": [
        "## 算法原理\n",
        "XGBoost是一种提升树模型，将许多树（CART树）模型集成在一起，形成一个很强的分类器。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L0e9Ojlf0T4U",
        "colab_type": "text"
      },
      "source": [
        "## 损失函数\n",
        "squre loss: $L(y_i, \\hat{y_i})=(y_i- \\hat{y_i})^2$\n",
        "Logistic loss: $L(y_i, \\hat{y_i})=y_iIn(1+e^{-\\hat{y_i}})+(1-y_i)In(1+e^{-\\hat{y_i}})$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y_98-18y0Y5T",
        "colab_type": "text"
      },
      "source": [
        "## 分裂结点算法\n",
        "1. 暴力枚举：遍历所有特征的所有可能的分割点，计算Gain值，选取最大(Feature, label)去分裂\n",
        "2. 近似方法：对于每个特征，只考察分位点，减少计算复杂度"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MPNHNhe_0WQ0",
        "colab_type": "text"
      },
      "source": [
        "## 正则化\n",
        "在XGBoost中，是将树的复杂度作为正则项加入，优化器在工作时会尽量不让这个树更复杂。\n",
        "\n",
        "在给出n个实例，m维特征的情况下，\n",
        "$\\hat{y_i}=\\phi(x_i)=\\sum^K_{k=1}f_k(x_i), f_k \\in F$\n",
        "\n",
        "$q(x)$代表一颗树，$W$代表叶子的分数，$f(x)$为样本实例的预测值，所以和CART的区别在于每个叶子节点有相应的权重$W_i$。为了学到模型需要的函数，需要定义正则化目标函数\n",
        "$L(\\phi)=\\sum_il(\\hat{y_i}, y_i)+\\sum_K\\Omega(f_k), where \\Omega(f)=\\gamma T+\\frac{1}{2}\\lambda ||w||^2$\n",
        "一种标准的正则化目标项= differentiable convex loss function + regularization，即损失函数+正则项。\n",
        "\n",
        "$L$衡量预测值与真实值的差异，$Ω$作为模型复杂度的惩罚项，对于树的叶子节点个数和叶子节点权重的正则，防止过拟合，(simple is perfect)，正则化项比RGF模型更加简单。\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Z45QHei0ewv",
        "colab_type": "text"
      },
      "source": [
        "### XGBoost 对缺失值的处理\n",
        "在XGBoost论文中关于缺失值的处理将其看作与稀疏矩阵的处理一样。在寻找split point的时候，不会对该特征为missing的样本进行遍历统计，只对该列特征值为non-missing的样本上对应的特征值进行遍历，通过这个技巧来减少了为稀疏离散特征寻找split point的时间开销。在逻辑实现上，为了保证完备性，会分别处理将missing该特征值的样本分配到左叶子结点和右叶子结点的两种情形，计算增益后选择增益大的方向进行分裂即可。可以为缺失值或者指定的值指定分支的默认方向，这能大大提升算法的效率。如果在训练中没有缺失值而在预测中出现缺失，那么会自动将缺失值的划分方向放到右子树。\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XPmbBnY70rdQ",
        "colab_type": "text"
      },
      "source": [
        "## 优缺点\n",
        "1. 优点 \n",
        "- （1）不仅是CART树，还可以线性分类器 \n",
        "- （2）引入正则化，提高模型的泛化能力 \n",
        "- （3）基于预排序算法，并行训练 \n",
        "- （4）对损失函数进行二阶泰勒展开，利用了一阶和二阶导数 \n",
        "2. 缺点 \n",
        "- （1）基于level-wise的分裂方式 \n",
        "- （2）预排序方法空间消耗比较大，不仅要保存特征值，也要保存特征的排序索引，同时时间消耗也大"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uZ1TiKL70cZW",
        "colab_type": "text"
      },
      "source": [
        "## 应用场景\n",
        "XGboost能够在一系列的问题上取得良好的效果，这些问题包括存销预测、物理事件分类、网页文本分类、顾客行为预测、点击率预测、动机探测、产品分类。多领域依赖数据分析和特征工程在这些结果中扮演重要的角色。XGBoost在所有场景中提供可扩展的功能，XGBoost可扩展性保证了相比其他系统更快速，XGBoost算法优势具体体现在：处理稀疏数据的新颖的树的学习算法、近似学习的分布式加权直方图。XGBoost能够基于外存的计算，保障了大数据的计算，使用少量的节点资源可处理大量的数据。XGBoost的主要贡献：\n",
        "\n",
        "- 构建了高可扩展的端到端的boosting系统。\n",
        "- 提出了具有合理理论支撑的分布分位调整框架。\n",
        "- 介绍了一个新颖的并行适应稀疏处理树学习算法。\n",
        "- 提出了基于缓存块的结构（cache-aware block structure）便于外存树（out-of-core tree）的学习。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uJl5VXy60pgT",
        "colab_type": "text"
      },
      "source": [
        "## Sklearn参数"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V7V-9hXK3Pa5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class xgboost.DMatrix(data, label=None, missing=None, weight=None, silent=False, \n",
        "                      feature_names=None, feature_types=None, nthread=None)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YZKK9iMt3Tp6",
        "colab_type": "text"
      },
      "source": [
        "`data`: DMatrix的数据源。当数据是字符串类型时，它表示路径libsvm格式txt文件，或者能xgboost读取的二进制文件\n",
        "\n",
        "`label`：训练数据的标签\n",
        "\n",
        "`missing`：需要以缺失值的形式表示的数据中的值\n",
        "\n",
        "`weight`：每个实例的权重\n",
        "\n",
        "`silent`：是否在构建期间打印信息\n",
        "\n",
        "`feature_names`：为特性设置名称\n",
        "\n",
        "`feature_types`：为特性设置类别\n",
        "\n",
        "`nthread`：从numpy从加载数据的线程数"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v8zqgBVr0lhz",
        "colab_type": "text"
      },
      "source": [
        "## 参考\n",
        "西瓜书\n",
        "\n",
        "cs229吴恩达机器学习课程\n",
        "\n",
        "李航统计学习\n",
        "\n",
        "https://www.cnblogs.com/wj-1314/p/9402324.html\n",
        "\n",
        "https://blog.csdn.net/a819825294/article/details/51206410\n",
        "\n",
        "https://zhuanlan.zhihu.com/p/38297689\n",
        "\n",
        "https://www.jiqizhixin.com/graph/technologies/ea0eb940-c873-42bc-a752-0a07f15a52c0\n",
        "\n",
        "公式推导参考：http://t.cn/EJ4F9Q0\n",
        "\n"
      ]
    }
  ]
}