{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rent Listing Inqueries 数据集的特征工程"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import必要的工具包，用于文件读取／特征编码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.feature_extraction.text import  CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "from scipy import sparse\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from nltk.metrics import distance as distance\n",
    "\n",
    "#from sklearn.cross_validation import StratifiedKFold\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "from MeanEncoder import MeanEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "数据文件路径和文件名"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#input data\n",
    "dpath = './data/'\n",
    "train = pd.read_json(dpath +\"RentListingInquries_train.json\")\n",
    "test = pd.read_json(dpath+\"RentListingInquries_test.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 标签interest_level"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 将类别型的标签interest_level编码为数字\n",
    "从前面的分析和常识来看，listing_id对确定interest_level没有用，去掉\n",
    "特征编码对训练集和测试集都要做，所以干脆将二者连起来一起处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y_map = {'low': 2, 'medium': 1, 'high': 0}\n",
    "train['interest_level'] = train['interest_level'].apply(lambda x: y_map[x])\n",
    "\n",
    "#y_train = train.interest_level.values\n",
    "y_train = train.interest_level\n",
    "train = train.drop(['listing_id', 'interest_level'], axis=1)\n",
    "\n",
    "listing_id = test.listing_id.values\n",
    "test = test.drop('listing_id', axis=1)\n",
    "\n",
    "ntrain = train.shape[0]\n",
    "train_test = pd.concat((train, test), axis=0).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10        1\n",
       "10000     2\n",
       "100004    0\n",
       "100007    2\n",
       "100013    2\n",
       "100014    1\n",
       "100016    2\n",
       "100020    2\n",
       "100026    1\n",
       "100027    2\n",
       "100030    2\n",
       "10004     2\n",
       "100044    0\n",
       "100048    2\n",
       "10005     2\n",
       "100051    1\n",
       "100052    2\n",
       "100053    2\n",
       "100055    2\n",
       "100058    2\n",
       "100062    2\n",
       "100063    1\n",
       "100065    2\n",
       "100066    2\n",
       "10007     1\n",
       "100071    2\n",
       "100075    1\n",
       "100076    2\n",
       "100079    0\n",
       "100081    2\n",
       "         ..\n",
       "99915     2\n",
       "99917     2\n",
       "99919     1\n",
       "99921     1\n",
       "99923     2\n",
       "99924     2\n",
       "99931     2\n",
       "99933     2\n",
       "99935     2\n",
       "99937     2\n",
       "9994      2\n",
       "99953     2\n",
       "99956     2\n",
       "99960     1\n",
       "99961     2\n",
       "99964     1\n",
       "99965     2\n",
       "99966     2\n",
       "99979     2\n",
       "99980     2\n",
       "99982     0\n",
       "99984     2\n",
       "99986     2\n",
       "99987     2\n",
       "99988     1\n",
       "9999      1\n",
       "99991     2\n",
       "99992     2\n",
       "99993     2\n",
       "99994     2\n",
       "Name: interest_level, dtype: int64"
      ]
     },
     "execution_count": 227,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## price, bathrooms, bedrooms\n",
    "数值型特征，+／-／*／ ／\n",
    "特征的单调变换对XGBoost不必要"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#remove some noise\n",
    "#ulimit = np.percentile(train_test.price.values, 99)\n",
    "train_test['price'].ix[train_test['price']>13000] = 13000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# remove some noise\n",
    "train_test.loc[train_test[\"bathrooms\"] == 112, \"bathrooms\"] = 1.5\n",
    "train_test.loc[train_test[\"bathrooms\"] == 10, \"bathrooms\"] = 1\n",
    "train_test.loc[train_test[\"bathrooms\"] == 20, \"bathrooms\"] = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_test['price_bathrooms'] =  (train_test[\"price\"])/ (train_test[\"bathrooms\"] +1.0)\n",
    "train_test['price_bedrooms'] =  (train_test[\"price\"])/ (train_test[\"bedrooms\"] +1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_test[\"room_diff\"] = train_test[\"bathrooms\"] - train_test[\"bedrooms\"]\n",
    "train_test[\"room_num\"] = train_test[\"bedrooms\"] + train_test[\"bathrooms\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 创建日期"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_test['Date'] = pd.to_datetime(train_test['created'])\n",
    "train_test['Year'] = train_test['Date'].dt.year\n",
    "train_test['Month'] = train_test['Date'].dt.month\n",
    "train_test['Day'] = train_test['Date'].dt.day\n",
    "train_test['Wday'] = train_test['Date'].dt.dayofweek\n",
    "train_test['Yday'] = train_test['Date'].dt.dayofyear\n",
    "train_test['hour'] = train_test['Date'].dt.hour\n",
    "\n",
    "train_test = train_test.drop(['Date', 'created'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# count of words present in description column #\n",
    "train_test[\"num_description_words\"] = train_test[\"description\"].apply(lambda x: len(x.split(\" \")))\n",
    "train_test = train_test.drop(['description'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## manager_id\n",
    "将manager分为几个等级\n",
    "top 1%， 2%， 5， 10， 15， 20， 25， 30， 50，"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "managers_count = train_test['manager_id'].value_counts()\n",
    "\n",
    "train_test['top_10_manager'] = train_test['manager_id'].apply(lambda x: 1 if x in managers_count.index.values[\n",
    "    managers_count.values >= np.percentile(managers_count.values, 90)] else 0)\n",
    "train_test['top_25_manager'] = train_test['manager_id'].apply(lambda x: 1 if x in managers_count.index.values[\n",
    "    managers_count.values >= np.percentile(managers_count.values, 75)] else 0)\n",
    "train_test['top_5_manager'] = train_test['manager_id'].apply(lambda x: 1 if x in managers_count.index.values[\n",
    "    managers_count.values >= np.percentile(managers_count.values, 95)] else 0)\n",
    "train_test['top_50_manager'] = train_test['manager_id'].apply(lambda x: 1 if x in managers_count.index.values[\n",
    "    managers_count.values >= np.percentile(managers_count.values, 50)] else 0)\n",
    "train_test['top_1_manager'] = train_test['manager_id'].apply(lambda x: 1 if x in managers_count.index.values[\n",
    "    managers_count.values >= np.percentile(managers_count.values, 99)] else 0)\n",
    "train_test['top_2_manager'] = train_test['manager_id'].apply(lambda x: 1 if x in managers_count.index.values[\n",
    "    managers_count.values >= np.percentile(managers_count.values, 98)] else 0)\n",
    "train_test['top_15_manager'] = train_test['manager_id'].apply(lambda x: 1 if x in managers_count.index.values[\n",
    "    managers_count.values >= np.percentile(managers_count.values, 85)] else 0)\n",
    "train_test['top_20_manager'] = train_test['manager_id'].apply(lambda x: 1 if x in managers_count.index.values[\n",
    "    managers_count.values >= np.percentile(managers_count.values, 80)] else 0)\n",
    "train_test['top_30_manager'] = train_test['manager_id'].apply(lambda x: 1 if x in managers_count.index.values[\n",
    "    managers_count.values >= np.percentile(managers_count.values, 70)] else 0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## building_id\n",
    "类似manager_id处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "buildings_count = train_test['building_id'].value_counts()\n",
    "\n",
    "train_test['top_10_building'] = train_test['building_id'].apply(lambda x: 1 if x in buildings_count.index.values[\n",
    "    buildings_count.values >= np.percentile(buildings_count.values, 90)] else 0)\n",
    "train_test['top_25_building'] = train_test['building_id'].apply(lambda x: 1 if x in buildings_count.index.values[\n",
    "    buildings_count.values >= np.percentile(buildings_count.values, 75)] else 0)\n",
    "train_test['top_5_building'] = train_test['building_id'].apply(lambda x: 1 if x in buildings_count.index.values[\n",
    "    buildings_count.values >= np.percentile(buildings_count.values, 95)] else 0)\n",
    "train_test['top_50_building'] = train_test['building_id'].apply(lambda x: 1 if x in buildings_count.index.values[\n",
    "    buildings_count.values >= np.percentile(buildings_count.values, 50)] else 0)\n",
    "train_test['top_1_building'] = train_test['building_id'].apply(lambda x: 1 if x in buildings_count.index.values[\n",
    "    buildings_count.values >= np.percentile(buildings_count.values, 99)] else 0)\n",
    "train_test['top_2_building'] = train_test['building_id'].apply(lambda x: 1 if x in buildings_count.index.values[\n",
    "    buildings_count.values >= np.percentile(buildings_count.values, 98)] else 0)\n",
    "train_test['top_15_building'] = train_test['building_id'].apply(lambda x: 1 if x in buildings_count.index.values[\n",
    "    buildings_count.values >= np.percentile(buildings_count.values, 85)] else 0)\n",
    "train_test['top_20_building'] = train_test['building_id'].apply(lambda x: 1 if x in buildings_count.index.values[\n",
    "    buildings_count.values >= np.percentile(buildings_count.values, 80)] else 0)\n",
    "train_test['top_30_building'] = train_test['building_id'].apply(lambda x: 1 if x in buildings_count.index.values[\n",
    "    buildings_count.values >= np.percentile(buildings_count.values, 70)] else 0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## photos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_test['photos_count'] = train_test['photos'].apply(lambda x: len(x))\n",
    "train_test.drop(['photos'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## latitude, longtitude\n",
    "聚类降维编码(#用训练数据训练，对训练数据和测试数据都做变换)\n",
    "到中心的距离（论坛上讨论到曼哈顿中心的距离更好）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    " # Clustering\n",
    "train_location = train_test.loc[:ntrain-1, ['latitude', 'longitude']]\n",
    "test_location = train_test.loc[ntrain:, ['latitude', 'longitude']]\n",
    "\n",
    "kmeans_cluster = KMeans(n_clusters=20)\n",
    "res = kmeans_cluster.fit(train_location)\n",
    "res = kmeans_cluster.predict( pd.concat((train_location, test_location), axis=0).reset_index(drop=True))\n",
    "\n",
    "train_test['cenroid'] = res\n",
    "\n",
    "# L1 distance\n",
    "center = [ train_location['latitude'].mean(), train_location['longitude'].mean()]\n",
    "train_test['distance'] = abs(train_test['latitude'] - center[0]) + abs(train_test['longitude'] - center[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## display_address"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_test['display_address'] = train_test['display_address'].apply(lambda x: x.lower().strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## street_address"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_test['street_address'] = train_test['street_address'].apply(lambda x: x.lower().strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 类别型特征\n",
    "LableEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "categoricals = ['building_id', 'manager_id', 'display_address', 'street_address']\n",
    "#categoricals = [x for x in train_test.columns if train_test[x].dtype == 'object']\n",
    "for feat in categoricals:\n",
    "    lbl = LabelEncoder()\n",
    "    lbl.fit(list(train_test[feat].values))\n",
    "    train_test[feat] = lbl.transform(list(train_test[feat].values))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "定义高基数类别型特征编码函数\n",
    "（manager_id, building_id, display_address,street_address ）\n",
    "对这些特征进行均值编码（该特征值在每个类别的概率，即原来的一维特征变成了C-1维特征，C为标签类别数目）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "me = MeanEncoder(categoricals)\n",
    "\n",
    "#trian\n",
    "#import pdb\n",
    "#pdb.set_trace()\n",
    "train_new = train_test.iloc[:ntrain, :]\n",
    "train_new_cat = me.fit_transform(train_new, y_train)\n",
    "\n",
    "#test\n",
    "test_new = train_test.iloc[ntrain:, :]\n",
    "test_new_cat = me.transform(test_new)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_test = pd.concat((train_new_cat, test_new_cat), axis=0).reset_index(drop=True)\n",
    "train_test.drop(categoricals, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## features\n",
    "描述特征文字长度\n",
    "特征中单词的词频，相当于以数据集features中出现的词语为字典的one-hot编码（虽然是词频，但在这个任务中每个单词）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_test['features_count'] = train_test['features'].apply(lambda x: len(x))\n",
    "train_test['features2'] = train_test['features']\n",
    "train_test['features2'] = train_test['features2'].apply(lambda x: ' '.join(x))\n",
    "\n",
    "c_vect = CountVectorizer(stop_words='english', max_features=200, ngram_range=(1, 1))\n",
    "c_vect_sparse = c_vect.fit_transform(train_test['features2'])\n",
    "c_vect_sparse_cols = c_vect.get_feature_names()\n",
    "\n",
    "train_test.drop(['features', 'features2'], axis=1, inplace=True)\n",
    "\n",
    "#hstack作为特征处理的最后一部，先将其他所有特征都转换成数值型特征才能处理\n",
    "train_test_sparse = sparse.hstack([train_test, c_vect_sparse]).tocsr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 特征处理结果存为文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10        1\n",
       "10000     2\n",
       "100004    0\n",
       "100007    2\n",
       "100013    2\n",
       "100014    1\n",
       "100016    2\n",
       "100020    2\n",
       "100026    1\n",
       "100027    2\n",
       "100030    2\n",
       "10004     2\n",
       "100044    0\n",
       "100048    2\n",
       "10005     2\n",
       "100051    1\n",
       "100052    2\n",
       "100053    2\n",
       "100055    2\n",
       "100058    2\n",
       "100062    2\n",
       "100063    1\n",
       "100065    2\n",
       "100066    2\n",
       "10007     1\n",
       "100071    2\n",
       "100075    1\n",
       "100076    2\n",
       "100079    0\n",
       "100081    2\n",
       "         ..\n",
       "99915     2\n",
       "99917     2\n",
       "99919     1\n",
       "99921     1\n",
       "99923     2\n",
       "99924     2\n",
       "99931     2\n",
       "99933     2\n",
       "99935     2\n",
       "99937     2\n",
       "9994      2\n",
       "99953     2\n",
       "99956     2\n",
       "99960     1\n",
       "99961     2\n",
       "99964     1\n",
       "99965     2\n",
       "99966     2\n",
       "99979     2\n",
       "99980     2\n",
       "99982     0\n",
       "99984     2\n",
       "99986     2\n",
       "99987     2\n",
       "99988     1\n",
       "9999      1\n",
       "99991     2\n",
       "99992     2\n",
       "99993     2\n",
       "99994     2\n",
       "Name: interest_level, dtype: int64"
      ]
     },
     "execution_count": 244,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#存为csv格式方便用excel查看\n",
    "train_test_new = pd.DataFrame(train_test_sparse.toarray())\n",
    "X_train = train_test_new.iloc[:ntrain, :]\n",
    "X_test = train_test_new.iloc[ntrain:, :]\n",
    "\n",
    "train_new = pd.concat((X_train, y_train), axis=1).reset_index(drop=True)\n",
    "train_new.to_csv(dpath + 'RentListingInquries_FE_train.csv', index=False)\n",
    "X_test.to_csv(dpath + 'RentListingInquries_FE_test.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from  scipy.io import mmwrite\n",
    "\n",
    "X_train_sparse = train_test_sparse[:ntrain, :]\n",
    "X_test_sparse = train_test_sparse[ntrain:, :]\n",
    "\n",
    "train_sparse = sparse.hstack([X_train_sparse, sparse.csr_matrix(y_train).T]).tocsr()\n",
    "\n",
    "mmwrite(dpath + 'RentListingInquries_FE_train.txt',train_sparse)\n",
    "mmwrite(dpath + 'RentListingInquries_FE_test.txt',X_test_sparse)\n",
    "\n",
    "#存为libsvm稀疏格式，直接调用XGBoost的话用稀疏格式更高效\n",
    "#from sklearn.datasets import dump_svmlight_file\n",
    "#dump_svmlight_file(, y_train, dpath + 'RentListingInquries_FE_train.txt',X_train_sparse) \n",
    "#dump_svmlight_file(X_test_sparse,  dpath + 'RentListingInquries_FE_test.txt') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_test_new = pd.DataFrame(train_test_sparse.toarray())\n",
    "X_train = train_test_new.iloc[:ntrain, :]\n",
    "X_test = train_test_new.iloc[ntrain:, :]\n",
    "\n",
    "train_new = pd.concat((X_train, y_train), axis=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
